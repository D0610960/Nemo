{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MTCNN\n",
    "對輸入圖像建立金字塔是爲了檢測不同尺度的人臉，通過3階段級聯(前者的輸出=後者的輸入)CNN完成對人臉由粗到細(coarse-to-fine)的檢測，前者往往先使用少量信息做個大致的判斷，快速將不是人臉的區域剔除，剩下可能包含人臉的區域交給後面更復雜的網絡，利用更多信息進一步篩選，這種由粗到細的方式在保證召回率的情況下可以大大提高篩選效率，最後輸出face的Bounding Box以及face的關鍵點(眼睛、鼻子、嘴)位置\n",
    "\n",
    "\n",
    "3-network\n",
    "P-Net：其實是個全卷積神經網絡（FCN），前向傳播得到的特徵圖在每個位置是個32維的特徵向量，用於判斷每個位置處約12×1212×12大小的區域內是否包含人臉，如果包含人臉，則迴歸出人臉的Bounding Box，進一步獲得Bounding Box對應到原圖中的區域，通過NMS保留分數最高的Bounding box以及移除重疊區域過大的Bounding Box\n",
    "\n",
    "O-Net：是單純的卷積神經網絡（CNN），先將P-Net認爲可能包含人臉的Bounding Box 雙線性插值到24×2424×24，輸入給O-Net，判斷是否包含人臉，如果包含人臉，也迴歸出Bounding Box，同樣經過NMS過濾\n",
    "\n",
    "R-Net：也是純粹的卷積神經網絡（CNN），將O-Net認爲可能包含人臉的Bounding Box 雙線性插值到48×4848×48，輸入給R-Net，進行人臉檢測和關鍵點提取\n",
    "\n",
    "\n",
    "重要參數\n",
    "threshold：人脸框得分阈值，三个网络可单独设定阈值，值设置太小，会有很多框通过，也就增加了计算量，可能导致最后不是人脸的框错认为人脸\n",
    "minsize：最小可檢測圖像，該值大小為控制圖像金字塔階層數的参数之一，越小，階層越多，計算越多\n",
    "factor ：生成图像金字塔时候的缩放系数, 范围(0,1)，可控制图像金字塔的阶层数的参数之一，越大，階層越多，計算越多\n",
    "\n",
    "输入图片的尺寸，minsize和factor共同影响了图像金字塔的阶层数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detect_face'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-93967c80689b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmisc\u001b[0m \u001b[1;31m# 圖像處理\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdetect_face\u001b[0m \u001b[1;31m# 自行下載.py檔\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'detect_face'"
     ]
    }
   ],
   "source": [
    "from scipy import misc # 圖像處理\n",
    "import tensorflow as tf\n",
    "import detect_face # 自行下載.py檔\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "minsize = 20 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ] # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "\n",
    "\n",
    "print('Creating networks and loading parameters')\n",
    "with tf.Graph().as_default():\n",
    "    # per_process_gpu_memory_fraction指定每個GPU process中使用顯卡內存上限，只能均匀地作用于所有GPU，無法對不同GPU設置不同上限\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.7) # 使用 70% GPU 記憶體\n",
    "    sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n",
    "    with sess.as_default():\n",
    "        pnet, rnet, onet = detect_face.create_mtcnn(sess, None)\n",
    "\n",
    "image_path = './p1.jpg'\n",
    "img = misc.imread(image_path)\n",
    "\n",
    "# 進行人臉檢測\n",
    "bounding_boxes, _ = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "nrof_faces = bounding_boxes.shape[0] # total numbers of face\n",
    "print('找到人臉數目為：{}'.format(nrof_faces))\n",
    "  \n",
    "print(bounding_boxes)\n",
    "\n",
    "crop_faces = []\n",
    "face_numbers = 0\n",
    "for face_position in bounding_boxes:\n",
    "    face_position=face_position.astype(int)\n",
    "    print(face_position[0:4])\n",
    "    cv2.rectangle(img, (face_position[0], face_position[1]), (face_position[2], face_position[3]), (0, 255, 0), 2)\n",
    "    #裁切\n",
    "    crop = img[face_position[1]:face_position[3], face_position[0]:face_position[2],]\n",
    "\n",
    "    crop = cv2.resize(crop, (250, 250), interpolation = cv2.INTER_CUBIC)\n",
    "    print(crop.shape)\n",
    "    crop_faces.append(crop)\n",
    "    plt.imshow(crop)\n",
    "    plt.show()\n",
    "    \n",
    "    crop = cv2.cvtColor(crop, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite('face' + str(face_numbers) + '.jpg', crop)\n",
    "    face_numbers += 1\n",
    "#original image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_face(img):\n",
    "    minsize = 20  # minimum size of face\n",
    "    threshold = [0.6, 0.7, 0.7]  # three steps's threshold\n",
    "    factor = 0.709  # scale factor\n",
    "    print('Creating networks and loading parameters')\n",
    "    with tf.Graph().as_default():\n",
    "        # gpu_memory_fraction = 1.0\n",
    "        # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "        # sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            pnet, rnet, onet = detect_face.create_mtcnn(sess, None)\n",
    "            bboxes, landmarks = detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n",
    "    landmarks = np.transpose(landmarks)\n",
    "    bboxes = bboxes.astype(int)\n",
    "    bboxes = [b[:4] for b in bboxes]\n",
    "    landmarks_list=[]\n",
    "    for landmark in landmarks:\n",
    "        face_landmarks = [[landmark[j], landmark[j + 5]] for j in range(5)]\n",
    "        landmarks_list.append(face_landmarks)\n",
    "    return bboxes,landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Facedetection:\n",
    "    def __init__(self):\n",
    "        self.minsize = 30  # minimum size of face\n",
    "        self.threshold = [0.6, 0.7, 0.7]  # three steps's threshold\n",
    "        self.factor = 0.709  # scale factor\n",
    "        print('Creating networks and loading parameters')\n",
    "        with tf.Graph().as_default():\n",
    "            # gpu_memory_fraction = 1.0\n",
    "            # gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n",
    "            # sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "            sess = tf.Session()\n",
    "            with sess.as_default():\n",
    "                self.pnet, self.rnet, self.onet = detect_face.create_mtcnn(sess, None)\n",
    "    def detect_face(self,image,fixed=None):\n",
    "        '''\n",
    "        mtcnn人脸检测，\n",
    "        PS：人脸检测获得bboxes并不一定是正方形的矩形框，参数fixed指定等宽或者等高的bboxes\n",
    "        :param image:\n",
    "        :param fixed:\n",
    "        :return:\n",
    "        '''\n",
    "        bboxes, landmarks = detect_face.detect_face(image, self.minsize, self.pnet, self.rnet, self.onet, self.threshold, self.factor)\n",
    "        landmarks_list = []\n",
    "        landmarks=np.transpose(landmarks)\n",
    "        bboxes=bboxes.astype(int)\n",
    "        bboxes = [b[:4] for b in bboxes]\n",
    "        for landmark in landmarks:\n",
    "            face_landmarks = [[landmark[j], landmark[j + 5]] for j in range(5)]\n",
    "            landmarks_list.append(face_landmarks)\n",
    "        if fixed is not None:\n",
    "            bboxes,landmarks_list=self.get_square_bboxes(bboxes, landmarks_list, fixed)\n",
    "        return bboxes,landmarks_list\n",
    " \n",
    "    def get_square_bboxes(self, bboxes, landmarks, fixed=\"height\"):\n",
    "        '''\n",
    "        获得等宽或者等高的bboxes\n",
    "        :param bboxes:\n",
    "        :param landmarks:\n",
    "        :param fixed: width or height\n",
    "        :return:\n",
    "        '''\n",
    "        new_bboxes = []\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            center_x, center_y = (int((x1 + x2) / 2), int((y1 + y2) / 2))\n",
    "            if fixed == \"height\":\n",
    "                dd = h / 2\n",
    "            elif fixed == 'width':\n",
    "                dd = w / 2\n",
    "            x11 = int(center_x - dd)\n",
    "            y11 = int(center_y - dd)\n",
    "            x22 = int(center_x + dd)\n",
    "            y22 = int(center_y + dd)\n",
    "            new_bbox = (x11, y11, x22, y22)\n",
    "            new_bboxes.append(new_bbox)\n",
    "        return new_bboxes, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class facenetEmbedding:\n",
    "    def __init__(self,model_path):\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Load the model\n",
    "        facenet.load_model(model_path)\n",
    "        # Get input and output tensors\n",
    "        self.images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        self.tf_embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        self.phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    " \n",
    "    def  get_embedding(self,images):\n",
    "        feed_dict = {self.images_placeholder: images, self.phase_train_placeholder: False}\n",
    "        embedding = self.sess.run(self.tf_embeddings, feed_dict=feed_dict)\n",
    "        return embedding\n",
    "    def free(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from utils import image_processing , file_processing,debug\n",
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    " \n",
    "resize_width = 160\n",
    "resize_height = 160\n",
    "\n",
    "def get_face_embedding(model_path,files_list, names_list):\n",
    "    '''\n",
    "    获得embedding数据\n",
    "    :param files_list: 图像列表\n",
    "    :param names_list: 与files_list一一的名称列表\n",
    "    :return:\n",
    "    '''\n",
    "    # 转换颜色空间RGB or BGR\n",
    "    colorSpace=\"RGB\"\n",
    "    # 初始化mtcnn人脸检测\n",
    "    face_detect = face_recognition.Facedetection()\n",
    "    # 初始化facenet\n",
    "    face_net = face_recognition.facenetEmbedding(model_path)\n",
    " \n",
    "    embeddings=[] # 用于保存人脸特征数据库\n",
    "    label_list=[] # 保存人脸label的名称，与embeddings一一对应\n",
    "    for image_path, name in zip(files_list, names_list):\n",
    "        print(\"processing image :{}\".format(image_path))\n",
    "        # image_path='E:/Face/dataset/bzl/subjectphoto_with_name/谢伟林_179_180.jpg'\n",
    "        image = image_processing.read_image_gbk(image_path, colorSpace=colorSpace)\n",
    "        # 进行人脸检测，获得bounding_box\n",
    "        bboxes, landmarks = face_detect.detect_face(image)\n",
    "        bboxes, landmarks =face_detect.get_square_bboxes(bboxes, landmarks,fixed=\"height\")\n",
    "        # image_processing.show_image_boxes(\"image\",image,bboxes)\n",
    "        if bboxes == [] or landmarks == []:\n",
    "            print(\"-----no face\")\n",
    "            continue\n",
    "        if len(bboxes) >= 2 or len(landmarks) >= 2:\n",
    "            print(\"-----image have {} faces\".format(len(bboxes)))\n",
    "            continue\n",
    "        # 获得人脸区域\n",
    "        face_images = image_processing.get_bboxes_image(image, bboxes, resize_height, resize_width)\n",
    "        # 人脸预处理，归一化\n",
    "        face_images = image_processing.get_prewhiten_images(face_images,normalization=True)\n",
    "        # 获得人脸特征\n",
    "        pred_emb = face_net.get_embedding(face_images)\n",
    "        embeddings.append(pred_emb)\n",
    "        # 可以选择保存image_list或者names_list作为人脸的标签\n",
    "        # 测试时建议保存image_list，这样方便知道被检测人脸与哪一张图片相似\n",
    "        # label_list.append(image_path)\n",
    "        label_list.append(name)\n",
    "    return embeddings,label_list\n",
    " \n",
    "def create_face_embedding(model_path,dataset_path,out_emb_path,out_filename):\n",
    "    '''\n",
    "    :param model_path: faceNet模型路径\n",
    "    :param dataset_path: 人脸数据库路径，每一类单独一个文件夹\n",
    "    :param out_emb_path: 输出embeddings的路径\n",
    "    :param out_filename: 输出与embeddings一一对应的标签\n",
    "    :return: None\n",
    "    '''\n",
    "    files_list,names_list=file_processing.gen_files_labels(dataset_path,postfix='jpg')\n",
    "    embeddings,label_list=get_face_embedding(model_path,files_list, names_list)\n",
    "    print(\"label_list:{}\".format(label_list))\n",
    "    print(\"have {} label\".format(len(label_list)))\n",
    " \n",
    "    embeddings=np.asarray(embeddings)\n",
    "    np.save(out_emb_path, embeddings)\n",
    "    file_processing.write_data(out_filename, label_list, model='w')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    model_path = 'models/20180408-102900'\n",
    "    dataset_path='dataset/images'\n",
    "    out_emb_path = 'dataset/emb/faceEmbedding.npy'\n",
    "    out_filename = 'dataset/emb/name.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "# initialise the detector class.\n",
    "detector = MTCNN()\n",
    "\n",
    "# load an image as an array\n",
    "image = face_recognition.load_image_file(\"face0.jpg\")\n",
    "\n",
    "# detect faces from input image.\n",
    "face_locations = detector.detect_faces(image)\n",
    "\n",
    "# draw bounding box and five facial landmarks of detected face\n",
    "for face in zip(face_locations):\n",
    "    (x, y, w, h) = face[0]['box']\n",
    "    landmarks = face[0]['keypoints']\n",
    "    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    for key, point in landmarks.items():\n",
    "        cv2.circle(image, point, 2, (255, 0, 0), 6)\n",
    "\n",
    "cv2.imshow('image',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load image and find face locations.\n",
    "image = face_recognition.load_image_file(\"face0.jpg\")\n",
    "face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "\n",
    "# detect 68-landmarks from image. This includes left eye, right eye, lips, eye brows, nose and chins\n",
    "face_landmarks = face_recognition.face_landmarks(image)\n",
    "\n",
    "'''\n",
    "Let's find and angle of the face. First calculate \n",
    "the center of left and right eye by using eye landmarks.\n",
    "'''\n",
    "leftEyePts = face_landmarks[0]['left_eye']\n",
    "rightEyePts = face_landmarks[0]['right_eye']\n",
    "\n",
    "leftEyeCenter = np.array(leftEyePts).mean(axis=0).astype(\"int\")\n",
    "rightEyeCenter = np.array(rightEyePts).mean(axis=0).astype(\"int\")\n",
    "\n",
    "leftEyeCenter = (leftEyeCenter[0],leftEyeCenter[1])\n",
    "rightEyeCenter = (rightEyeCenter[0],rightEyeCenter[1])\n",
    "\n",
    "# draw the circle at centers and line connecting to them\n",
    "cv2.circle(image, leftEyeCenter, 2, (255, 0, 0), 10)\n",
    "cv2.circle(image, rightEyeCenter, 2, (255, 0, 0), 10)\n",
    "cv2.line(image, leftEyeCenter, rightEyeCenter, (255,0,0), 10)\n",
    "\n",
    "# find and angle of line by using slop of the line.\n",
    "dY = rightEyeCenter[1] - leftEyeCenter[1]\n",
    "dX = rightEyeCenter[0] - leftEyeCenter[0]\n",
    "angle = np.degrees(np.arctan2(dY, dX))\n",
    "\n",
    "# to get the face at the center of the image,\n",
    "# set desired left eye location. Right eye location \n",
    "# will be found out by using left eye location.\n",
    "# this location is in percentage.\n",
    "desiredLeftEye=(0.35, 0.35)\n",
    "#Set the croped image(face) size after rotaion.\n",
    "desiredFaceWidth = 128\n",
    "desiredFaceHeight = 128\n",
    "\n",
    "desiredRightEyeX = 1.0 - desiredLeftEye[0]\n",
    " \n",
    "# determine the scale of the new resulting image by taking\n",
    "# the ratio of the distance between eyes in the *current*\n",
    "# image to the ratio of distance between eyes in the\n",
    "# *desired* image\n",
    "dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "desiredDist = (desiredRightEyeX - desiredLeftEye[0])\n",
    "desiredDist *= desiredFaceWidth\n",
    "scale = desiredDist / dist\n",
    "\n",
    "# compute center (x, y)-coordinates (i.e., the median point)\n",
    "# between the two eyes in the input image\n",
    "eyesCenter = ((leftEyeCenter[0] + rightEyeCenter[0]) // 2,\n",
    "    (leftEyeCenter[1] + rightEyeCenter[1]) // 2)\n",
    "\n",
    "# grab the rotation matrix for rotating and scaling the face\n",
    "M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
    "\n",
    "# update the translation component of the matrix\n",
    "tX = desiredFaceWidth * 0.5\n",
    "tY = desiredFaceHeight * desiredLeftEye[1]\n",
    "M[0, 2] += (tX - eyesCenter[0])\n",
    "M[1, 2] += (tY - eyesCenter[1])\n",
    "\n",
    "# apply the affine transformation\n",
    "(w, h) = (desiredFaceWidth, desiredFaceHeight)\n",
    "(y2,x2,y1,x1) = face_locations[0] \n",
    "        \n",
    "output = cv2.warpAffine(image, M, (w, h),\n",
    "    flags=cv2.INTER_CUBIC)\n",
    "\n",
    "cv2.imshow('image',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import time\n",
    "from imutils import paths\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "knownFace = \"face0.jpg\"\n",
    "image = face_recognition.load_image_file(knownFace)\n",
    "face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "face_landmarks = face_recognition.face_landmarks(image)\n",
    "\n",
    "# after alignment we have to resize the image so we have to give \n",
    "# width and height of the output aligned face.\n",
    "(top,right,bottom,left) = face_locations[0] \n",
    "desiredWidth = (right-left) \n",
    "desiredHeight = (bottom-top)\n",
    "\n",
    "# use the code snippet for face alignment (from the previous section) \n",
    "# and create a function alignFace. set the desiredWidth and desiredHeight\n",
    "# to face width and height\n",
    "align_f = alignFace(image, face_locations, face_landmarks, desiredWidth, desiredHeight)\n",
    "\n",
    "# calculate face encodings of align face. It is array of 128 length.\n",
    "known_face_encoding = face_recognition.face_encodings(align_f, num_jitters=10)[0]\n",
    "\n",
    "unknownFace = \"face1.jpg\"\n",
    "image = face_recognition.load_image_file(unknownFace)\n",
    "face_locations = face_recognition.face_locations(image, model=\"hog\")\n",
    "face_landmarks = face_recognition.face_landmarks(image)\n",
    "\n",
    "# after alignment we have to resize the image so we have to give \n",
    "# width and height of the output aligned face.\n",
    "(top,right,bottom,left) = face_locations[0] \n",
    "desiredWidth = (right-left) \n",
    "desiredHeight = (bottom-top)\n",
    "\n",
    "# use the code snippet for face alignment (from the previous section) \n",
    "# and create a function alignFace. set the desiredWidth and desiredHeight\n",
    "# to face width and height\n",
    "align_f = alignFace(image, face_locations, face_landmarks, desiredWidth, desiredHeight)\n",
    "\n",
    "# calculate face encodings of align face. It is array of 128 length.\n",
    "unknown_face_encoding = face_recognition.face_encodings(align_f, num_jitters=10)[0]\n",
    "\n",
    "# calculate the distance between known and unknown face. \n",
    "# distance range is 0 to 1. If two faces are maching then value\n",
    "# of distance is near to zeor else it is much away from zero or near\n",
    "# to one.\n",
    "distance = face_recognition.face_distance([known_face_encoding], unknown_face_encoding)[0]\n",
    "print(\"Distance : {}\".format(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(image):\n",
    "    minsize = 20  # minimum size of face\n",
    "    threshold = [0.6, 0.7, 0.7]  # three steps‘s threshold\n",
    "    factor = 0.709  # scale factor\n",
    "\n",
    "    # detect with RGB image\n",
    "    h, w = image.shape[:2]\n",
    "    bounding_boxes, _ = align.detect_face.detect_face(image, minsize, pnet, rnet, onet, threshold, factor)\n",
    "    if len(bounding_boxes) < 1:\n",
    "        print(\"can‘t detect face in the frame\")\n",
    "        return None\n",
    "    print(\"num %d faces detected\"% len(bounding_boxes))\n",
    "    bgr = cv.cvtColor(image, cv.COLOR_RGB2BGR)\n",
    "    for i in range(len(bounding_boxes)):\n",
    "        det = np.squeeze(bounding_boxes[i, 0:4])\n",
    "        bb = np.zeros(4, dtype=np.int32)\n",
    "        # x1, y1, x2, y2\n",
    "        bb[0] = np.maximum(det[0] - margin / 2, 0)\n",
    "        bb[1] = np.maximum(det[1] - margin / 2, 0)\n",
    "        bb[2] = np.minimum(det[2] + margin / 2, w)\n",
    "        bb[3] = np.minimum(det[3] + margin / 2, h)\n",
    "        cv.rectangle(bgr, (bb[0], bb[1]), (bb[2], bb[3]), (0, 0, 255), 2, 8, 0)\n",
    "    cv.imshow(\"detected faces\", bgr)\n",
    "    return bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "out = cv.VideoWriter(\"D:/mtcnn_demo.mp4\", cv.VideoWriter_fourcc('D', 'I', 'V', 'X'), 15, (np.int(width), np.int(height)), True)\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if ret is True:\n",
    "        frame = cv.flip(frame, 1)\n",
    "        cv.imshow(\"frame\", frame)\n",
    "        rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        result = detection(rgb)\n",
    "\n",
    "        out.write(result)\n",
    "        c = cv.waitKey(10)\n",
    "        if c == 27:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    grab, frame = camera.read()\n",
    "    img = cv2.resize(frame, (320,180))\n",
    "\n",
    "    t1 = time.time()\n",
    "    results = detector.detect_face(img)\n",
    "    print('time: ',time.time() - t1)\n",
    "\n",
    "    if results is None:\n",
    "        continue\n",
    "\n",
    "    total_boxes = results[0]\n",
    "    points = results[1]\n",
    "\n",
    "    draw = img.copy()\n",
    "    for b in total_boxes:\n",
    "        cv2.rectangle(draw, (int(b[0]), int(b[1])), (int(b[2]), int(b[3])), (255, 255, 255))\n",
    "\n",
    "    for p in points:\n",
    "        for i in range(5):\n",
    "            cv2.circle(draw, (p[i], p[i + 5]), 1, (255, 0, 0), 2)\n",
    "    cv2.imshow(\"detection result\", draw)\n",
    "    cv2.waitKey(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
